version: '3.9'

x-logging: &default-logging
  driver: json-file
  options:
    max-size: "10m"
    max-file: "3"

networks:
  bev_osint:
    driver: bridge
    ipam:
      config:
        - subnet: 172.21.0.0/16

volumes:
  postgres_data:
  neo4j_data:
  redis_data:
  elasticsearch_data:
  influxdb_data:
  kafka_data:
  zookeeper_data:
  rabbitmq_data:
  tor_data:
  intelowl_postgres_data:
  intelowl_static_data:
  # Phase 1 volumes
  prometheus_data:
  grafana_data:
  airflow_logs:
  airflow_dags:
  airflow_plugins:
  airflow_config:
  # Phase 2 volumes
  ocr_data:
  document_analyzer_data:
  # Phase 3 volumes
  swarm_data:
  memory_data:
  code_data:
  tool_data:
  # Phase 4 volumes
  vault_data:
  guardian_data:
  tor_nodes_data:
  ids_data:
  security_logs:
  # Phase 5 volumes
  autonomous_data:
  live2d_data:
  avatar_assets:
  logs:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /home/starlord/Projects/Bev/logs

services:
  # PostgreSQL with pgvector
  postgres:
    image: pgvector/pgvector:pg16
    platform: linux/amd64
    container_name: bev_postgres
    restart: always
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_MULTIPLE_DATABASES: osint,intelowl,breach_data,crypto_analysis
      POSTGRES_HOST_AUTH_METHOD: md5
      SHARED_PRELOAD_LIBRARIES: pg_stat_statements,pgvector
      MAX_CONNECTIONS: 500
      SHARED_BUFFERS: 2GB
      EFFECTIVE_CACHE_SIZE: 6GB
      MAINTENANCE_WORK_MEM: 512MB
      WORK_MEM: 32MB
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init_scripts/postgres_init.sql:/docker-entrypoint-initdb.d/init.sql:ro
      - logs:/var/log/postgresql
    ports:
      - "5432:5432"
    networks:
      bev_osint:
        ipv4_address: 172.21.0.2
    logging: *default-logging
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER}"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Neo4j Graph Database
  neo4j:
    image: neo4j:5.14-enterprise
    platform: linux/amd64
    container_name: bev_neo4j
    restart: always
    environment:
      NEO4J_AUTH: ${NEO4J_USER}/${NEO4J_PASSWORD}
      NEO4J_ACCEPT_LICENSE_AGREEMENT: "yes"
      NEO4J_server_memory_heap_initial__size: 2G
      NEO4J_server_memory_heap_max__size: 4G
      NEO4J_server_memory_pagecache__size: 2G
      NEO4J_dbms_security_procedures_unrestricted: apoc.*,gds.*
      NEO4J_dbms_security_procedures_allowlist: apoc.*,gds.*
      NEO4J_apoc_export_file_enabled: "true"
      NEO4J_apoc_import_file_enabled: "true"
      NEO4J_PLUGINS: '["apoc", "graph-data-science"]'
      NEO4J_dbms_connector_bolt_listen__address: 0.0.0.0:7687
      NEO4J_dbms_connector_http_listen__address: 0.0.0.0:7474
      NEO4J_dbms_connector_https_listen__address: 0.0.0.0:7473
    volumes:
      - neo4j_data:/data
      - ./init_scripts/neo4j_init.cypher:/import/init.cypher:ro
      - logs:/logs
    ports:
      - "7474:7474"  # HTTP
      - "7687:7687"  # Bolt
      - "7473:7473"  # HTTPS
    networks:
      bev_osint:
        ipv4_address: 172.21.0.3
    logging: *default-logging

  # Redis Cluster Node 1
  redis-node-1:
    image: redis:7-alpine
    platform: linux/amd64
    container_name: bev_redis_1
    restart: always
    command: >
      redis-server
      --port 7001
      --cluster-enabled yes
      --cluster-config-file nodes.conf
      --cluster-node-timeout 5000
      --appendonly yes
      --requirepass ${REDIS_PASSWORD}
      --masterauth ${REDIS_PASSWORD}
      --maxmemory 2gb
      --maxmemory-policy allkeys-lru
    volumes:
      - ./redis/node1:/data
      - logs:/var/log/redis
    ports:
      - "7001:7001"
      - "17001:17001"
    networks:
      bev_osint:
        ipv4_address: 172.21.0.4
    logging: *default-logging

  # Redis Cluster Node 2
  redis-node-2:
    image: redis:7-alpine
    platform: linux/amd64
    container_name: bev_redis_2
    restart: always
    command: >
      redis-server
      --port 7002
      --cluster-enabled yes
      --cluster-config-file nodes.conf
      --cluster-node-timeout 5000
      --appendonly yes
      --requirepass ${REDIS_PASSWORD}
      --masterauth ${REDIS_PASSWORD}
      --maxmemory 2gb
      --maxmemory-policy allkeys-lru
    volumes:
      - ./redis/node2:/data
      - logs:/var/log/redis
    ports:
      - "7002:7002"
      - "17002:17002"
    networks:
      bev_osint:
        ipv4_address: 172.21.0.5
    logging: *default-logging

  # Redis Cluster Node 3
  redis-node-3:
    image: redis:7-alpine
    platform: linux/amd64
    container_name: bev_redis_3
    restart: always
    command: >
      redis-server
      --port 7003
      --cluster-enabled yes
      --cluster-config-file nodes.conf
      --cluster-node-timeout 5000
      --appendonly yes
      --requirepass ${REDIS_PASSWORD}
      --masterauth ${REDIS_PASSWORD}
      --maxmemory 2gb
      --maxmemory-policy allkeys-lru
    volumes:
      - ./redis/node3:/data
      - logs:/var/log/redis
    ports:
      - "7003:7003"
      - "17003:17003"
    networks:
      bev_osint:
        ipv4_address: 172.21.0.6
    logging: *default-logging

  # Redis Standalone for IntelOwl
  redis:
    image: redis:7-alpine
    platform: linux/amd64
    container_name: bev_redis_standalone
    restart: always
    command: redis-server --requirepass ${REDIS_PASSWORD} --maxmemory 1gb --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"
    networks:
      bev_osint:
        ipv4_address: 172.21.0.7
    logging: *default-logging

  # RabbitMQ Node 1
  rabbitmq-1:
    image: rabbitmq:3-management
    platform: linux/amd64
    container_name: bev_rabbitmq_1
    hostname: rabbitmq-1
    restart: always
    environment:
      RABBITMQ_DEFAULT_USER: ${RABBITMQ_USER}
      RABBITMQ_DEFAULT_PASS: ${RABBITMQ_PASSWORD}
      RABBITMQ_ERLANG_COOKIE: BevRabbitClusterCookie2024
      RABBITMQ_NODENAME: rabbit@rabbitmq-1
      RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS: -rabbit disk_free_limit 2147483648
    volumes:
      - ./rabbitmq/node1:/var/lib/rabbitmq
      - logs:/var/log/rabbitmq
    ports:
      - "5672:5672"
      - "15672:15672"
    networks:
      bev_osint:
        ipv4_address: 172.21.0.8
    logging: *default-logging

  # RabbitMQ Node 2
  rabbitmq-2:
    image: rabbitmq:3-management
    platform: linux/amd64
    container_name: bev_rabbitmq_2
    hostname: rabbitmq-2
    restart: always
    environment:
      RABBITMQ_DEFAULT_USER: ${RABBITMQ_USER}
      RABBITMQ_DEFAULT_PASS: ${RABBITMQ_PASSWORD}
      RABBITMQ_ERLANG_COOKIE: BevRabbitClusterCookie2024
      RABBITMQ_NODENAME: rabbit@rabbitmq-2
      RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS: -rabbit disk_free_limit 2147483648
    volumes:
      - ./rabbitmq/node2:/var/lib/rabbitmq
      - logs:/var/log/rabbitmq
    ports:
      - "5673:5672"
      - "15673:15672"
    networks:
      bev_osint:
        ipv4_address: 172.21.0.9
    logging: *default-logging

  # RabbitMQ Node 3
  rabbitmq-3:
    image: rabbitmq:3-management
    platform: linux/amd64
    container_name: bev_rabbitmq_3
    hostname: rabbitmq-3
    restart: always
    environment:
      RABBITMQ_DEFAULT_USER: ${RABBITMQ_USER}
      RABBITMQ_DEFAULT_PASS: ${RABBITMQ_PASSWORD}
      RABBITMQ_ERLANG_COOKIE: BevRabbitClusterCookie2024
      RABBITMQ_NODENAME: rabbit@rabbitmq-3
      RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS: -rabbit disk_free_limit 2147483648
    volumes:
      - ./rabbitmq/node3:/var/lib/rabbitmq
      - logs:/var/log/rabbitmq
    ports:
      - "5674:5672"
      - "15674:15672"
    networks:
      bev_osint:
        ipv4_address: 172.21.0.10
    logging: *default-logging

  # Zookeeper for Kafka
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    platform: linux/amd64
    container_name: bev_zookeeper
    restart: always
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_SYNC_LIMIT: 2
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_MAX_CLIENT_CNXNS: 100
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
      - ./zookeeper/log:/var/lib/zookeeper/log
      - logs:/var/log/zookeeper
    ports:
      - "2181:2181"
    networks:
      bev_osint:
        ipv4_address: 172.21.0.11
    logging: *default-logging

  # Kafka Broker 1
  kafka-1:
    image: confluentinc/cp-kafka:7.5.0
    platform: linux/amd64
    container_name: bev_kafka_1
    restart: always
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-1:9092,EXTERNAL://0.0.0.0:19092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_NUM_PARTITIONS: 3
    volumes:
      - ./kafka/broker1:/var/lib/kafka/data
      - logs:/var/log/kafka
    ports:
      - "19092:19092"
    networks:
      bev_osint:
        ipv4_address: 172.21.0.12
    logging: *default-logging

  # Kafka Broker 2
  kafka-2:
    image: confluentinc/cp-kafka:7.5.0
    platform: linux/amd64
    container_name: bev_kafka_2
    restart: always
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-2:9092,EXTERNAL://0.0.0.0:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_NUM_PARTITIONS: 3
    volumes:
      - ./kafka/broker2:/var/lib/kafka/data
      - logs:/var/log/kafka
    ports:
      - "29092:29092"
    networks:
      bev_osint:
        ipv4_address: 172.21.0.13
    logging: *default-logging

  # Kafka Broker 3
  kafka-3:
    image: confluentinc/cp-kafka:7.5.0
    platform: linux/amd64
    container_name: bev_kafka_3
    restart: always
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-3:9092,EXTERNAL://0.0.0.0:39092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_NUM_PARTITIONS: 3
    volumes:
      - ./kafka/broker3:/var/lib/kafka/data
      - logs:/var/log/kafka
    ports:
      - "39092:39092"
    networks:
      bev_osint:
        ipv4_address: 172.21.0.14
    logging: *default-logging

  # Elasticsearch
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    platform: linux/amd64
    container_name: bev_elasticsearch
    restart: always
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - xpack.security.enrollment.enabled=false
      - "ES_JAVA_OPTS=-Xms2g -Xmx2g"
      - cluster.name=bev-osint-cluster
      - bootstrap.memory_lock=true
      - indices.query.bool.max_clause_count=10000
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
      - logs:/usr/share/elasticsearch/logs
    ports:
      - "9200:9200"
      - "9300:9300"
    networks:
      bev_osint:
        ipv4_address: 172.21.0.15
    logging: *default-logging
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9200"]
      interval: 30s
      timeout: 10s
      retries: 5

  # InfluxDB
  influxdb:
    image: influxdb:2.7
    platform: linux/amd64
    container_name: bev_influxdb
    restart: always
    environment:
      DOCKER_INFLUXDB_INIT_MODE: setup
      DOCKER_INFLUXDB_INIT_USERNAME: bev
      DOCKER_INFLUXDB_INIT_PASSWORD: BevMetrics2024
      DOCKER_INFLUXDB_INIT_ORG: bev-osint
      DOCKER_INFLUXDB_INIT_BUCKET: metrics
      DOCKER_INFLUXDB_INIT_RETENTION: 30d
      DOCKER_INFLUXDB_INIT_ADMIN_TOKEN: ${INFLUXDB_TOKEN}
      INFLUXDB_DATA_QUERY_LOG_ENABLED: "false"
      INFLUXDB_HTTP_LOG_ENABLED: "false"
    volumes:
      - influxdb_data:/var/lib/influxdb2
      - ./influxdb/config:/etc/influxdb2
      - logs:/var/log/influxdb
    ports:
      - "8086:8086"
    networks:
      bev_osint:
        ipv4_address: 172.21.0.16
    logging: *default-logging

  # Tor SOCKS5 Proxy with Control Port
  tor:
    image: dperson/torproxy:latest
    platform: linux/amd64
    container_name: bev_tor
    restart: always
    environment:
      TORUSER: bev
      PASSWORD: ${TOR_CONTROL_PASSWORD}
      TOR_NewCircuitPeriod: 600
      TOR_MaxCircuitDirtiness: 600
      TOR_NumEntryGuards: 6
      TOR_ControlPort: 9051
      TOR_HashedControlPassword: 16:872860B76453A77D60CA2BB8C1A7042072093276A3D701AD684053EC4C
    volumes:
      - tor_data:/var/lib/tor
      - ./tor/torrc:/etc/tor/torrc:ro
      - logs:/var/log/tor
    ports:
      - "9050:9050"  # SOCKS5
      - "9051:9051"  # Control
      - "8118:8118"  # Privoxy HTTP
    networks:
      bev_osint:
        ipv4_address: 172.21.0.17
    logging: *default-logging

  # IntelOwl Postgres
  intelowl-postgres:
    image: postgres:16-alpine
    container_name: bev_intelowl_postgres
    restart: always
    environment:
      POSTGRES_DB: ${INTELOWL_POSTGRES_DB}
      POSTGRES_USER: ${INTELOWL_POSTGRES_USER}
      POSTGRES_PASSWORD: ${INTELOWL_POSTGRES_PASSWORD}
    volumes:
      - intelowl_postgres_data:/var/lib/postgresql/data
      - ./intelowl/sql_init:/docker-entrypoint-initdb.d
    networks:
      bev_osint:
        ipv4_address: 172.21.0.18
    logging: *default-logging

  # IntelOwl Celery Beat
  intelowl-celery-beat:
    image: intelowlproject/intelowl:v5.2.0
    platform: linux/amd64
    container_name: bev_intelowl_celery_beat
    restart: always
    depends_on:
      - intelowl-postgres
      - redis
      - rabbitmq-1
    environment:
      - DJANGO_SECRET_KEY=${DJANGO_SECRET_KEY}
      - POSTGRES_HOST=${INTELOWL_POSTGRES_HOST}
      - POSTGRES_DB=${INTELOWL_POSTGRES_DB}
      - POSTGRES_USER=${INTELOWL_POSTGRES_USER}
      - POSTGRES_PASSWORD=${INTELOWL_POSTGRES_PASSWORD}
      - REDIS_URL=redis://:${REDIS_PASSWORD}@redis:6379/1
      - CELERY_BROKER_URL=amqp://${RABBITMQ_USER}:${RABBITMQ_PASSWORD}@rabbitmq-1:5672
      - DISABLE_AUTHENTICATION_CHECKS=True
      - DISABLE_PERMISSIONS_CHECKS=True
      - DISABLE_THROTTLING=True
    volumes:
      - ./intelowl/custom_analyzers:/opt/deploy/intelowl/custom_analyzers:ro
      - ./intelowl/custom_connectors:/opt/deploy/intelowl/custom_connectors:ro
      - /home/starlord/Bev/src:/opt/bev_src:ro
      - intelowl_static_data:/opt/deploy/static
      - logs:/var/log/intelowl
    command: celery -A intel_owl beat -l info --scheduler django_celery_beat.schedulers:DatabaseScheduler
    networks:
      bev_osint:
        ipv4_address: 172.21.0.19
    logging: *default-logging

  # IntelOwl Celery Worker
  intelowl-celery-worker:
    image: intelowlproject/intelowl:v5.2.0
    platform: linux/amd64
    restart: always
    depends_on:
      - intelowl-postgres
      - redis
      - rabbitmq-1
    environment:
      - DJANGO_SECRET_KEY=${DJANGO_SECRET_KEY}
      - POSTGRES_HOST=${INTELOWL_POSTGRES_HOST}
      - POSTGRES_DB=${INTELOWL_POSTGRES_DB}
      - POSTGRES_USER=${INTELOWL_POSTGRES_USER}
      - POSTGRES_PASSWORD=${INTELOWL_POSTGRES_PASSWORD}
      - REDIS_URL=redis://:${REDIS_PASSWORD}@redis:6379/1
      - CELERY_BROKER_URL=amqp://${RABBITMQ_USER}:${RABBITMQ_PASSWORD}@rabbitmq-1:5672
      - DISABLE_AUTHENTICATION_CHECKS=True
      - DISABLE_PERMISSIONS_CHECKS=True
      - DISABLE_THROTTLING=True
      - HTTP_PROXY=socks5://tor:9050
      - HTTPS_PROXY=socks5://tor:9050
      - TOR_PROXY=${TOR_PROXY}
      - NEO4J_URI=${NEO4J_URI}
      - NEO4J_USER=${NEO4J_USER}
      - NEO4J_PASSWORD=${NEO4J_PASSWORD}
      - DEHASHED_API_KEY=${DEHASHED_API_KEY}
      - DEHASHED_EMAIL=${DEHASHED_EMAIL}
      - SNUSBASE_API_KEY=${SNUSBASE_API_KEY}
      - WELEAKINFO_API_KEY=${WELEAKINFO_API_KEY}
      - SHODAN_API_KEY=${SHODAN_API_KEY}
      - VIRUSTOTAL_API_KEY=${VIRUSTOTAL_API_KEY}
    volumes:
      - ./intelowl/custom_analyzers:/opt/deploy/intelowl/custom_analyzers:ro
      - ./intelowl/custom_connectors:/opt/deploy/intelowl/custom_connectors:ro
      - /home/starlord/Bev/src:/opt/bev_src:ro
      - intelowl_static_data:/opt/deploy/static
      - logs:/var/log/intelowl
    command: celery -A intel_owl worker -l info -c ${WORKERS} --max-tasks-per-child 100
    networks:
      bev_osint:
        ipv4_address: 172.21.0.20
    logging: *default-logging
    deploy:
      replicas: 4
      resources:
        limits:
          memory: 4G

  # IntelOwl Django/API
  intelowl-django:
    image: intelowlproject/intelowl:v5.2.0
    platform: linux/amd64
    container_name: bev_intelowl_django
    restart: always
    depends_on:
      - intelowl-postgres
      - redis
      - rabbitmq-1
    environment:
      - DJANGO_ALLOWED_HOSTS=0.0.0.0,*
      - DJANGO_SECRET_KEY=${DJANGO_SECRET_KEY}
      - POSTGRES_HOST=${INTELOWL_POSTGRES_HOST}
      - POSTGRES_DB=${INTELOWL_POSTGRES_DB}
      - POSTGRES_USER=${INTELOWL_POSTGRES_USER}
      - POSTGRES_PASSWORD=${INTELOWL_POSTGRES_PASSWORD}
      - REDIS_URL=redis://:${REDIS_PASSWORD}@redis:6379/1
      - CELERY_BROKER_URL=amqp://${RABBITMQ_USER}:${RABBITMQ_PASSWORD}@rabbitmq-1:5672
      - DISABLE_AUTHENTICATION_CHECKS=True
      - DISABLE_PERMISSIONS_CHECKS=True
      - DISABLE_THROTTLING=True
      - DJANGO_ALLOWED_HOSTS=${DJANGO_ALLOWED_HOSTS}
      - DJANGO_DEBUG=${DJANGO_DEBUG}
      - DEFAULT_FROM_EMAIL=bev-osint@localhost
      - DEFAULT_EMAIL=bev@localhost
    volumes:
      - ./intelowl/custom_analyzers:/opt/deploy/intelowl/custom_analyzers:ro
      - ./intelowl/custom_connectors:/opt/deploy/intelowl/custom_connectors:ro
      - /home/starlord/Bev/src:/opt/bev_src:ro
      - intelowl_static_data:/opt/deploy/static
      - ./intelowl/dark_theme.css:/opt/deploy/static/css/custom.css:ro
      - logs:/var/log/intelowl
    command: >
      sh -c "
      python manage.py migrate &&
      python manage.py collectstatic --noinput &&
      python manage.py createsuperuser --noinput --username admin --email admin@localhost || true &&
      gunicorn intel_owl.wsgi:application --bind 0.0.0.0:8000 --workers ${WORKERS} --threads ${THREADS_PER_WORKER} --timeout ${REQUEST_TIMEOUT} --access-logfile - --error-logfile -"
    ports:
      - "8000:8000"
    networks:
      bev_osint:
        ipv4_address: 172.21.0.21
    logging: *default-logging
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  # IntelOwl Nginx with Cytoscape
  intelowl-nginx:
    image: nginx:alpine
    platform: linux/amd64
    container_name: bev_intelowl_nginx
    restart: always
    depends_on:
      - intelowl-django
    volumes:
      - ./intelowl/nginx.conf:/etc/nginx/nginx.conf:ro
      - intelowl_static_data:/usr/share/nginx/html/static:ro
      - ./cytoscape:/usr/share/nginx/html/cytoscape:ro
      - ./intelowl/dark_theme.css:/usr/share/nginx/html/static/css/dark_theme.css:ro
      - logs:/var/log/nginx
    ports:
      - "80:80"
      - "443:443"
    networks:
      bev_osint:
        ipv4_address: 172.21.0.22
    logging: *default-logging
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:80"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Cytoscape.js Server (Custom Node.js App)
  cytoscape-server:
    build:
      context: ./cytoscape
      dockerfile: Dockerfile
    container_name: bev_cytoscape_server
    restart: always
    depends_on:
      - neo4j
      - postgres
    environment:
      NODE_ENV: production
      NEO4J_URI: ${NEO4J_URI}
      NEO4J_USER: ${NEO4J_USER}
      NEO4J_PASSWORD: ${NEO4J_PASSWORD}
      POSTGRES_URI: ${POSTGRES_URI}
      PORT: 3000
    volumes:
      - ./cytoscape:/app
      - logs:/app/logs
    ports:
      - "3000:3000"
    networks:
      bev_osint:
        ipv4_address: 172.21.0.23
    logging: *default-logging

  # ================================================
  # PHASE 1: Monitoring & Orchestration Layer
  # ================================================

  # Prometheus
  prometheus:
    image: prom/prometheus:v2.47.0
    platform: linux/amd64
    container_name: bev_prometheus
    restart: always
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./prometheus/rules:/etc/prometheus/rules:ro
      - prometheus_data:/prometheus
      - logs:/var/log/prometheus
    environment:
      - --web.listen-address=0.0.0.0:9090
    ports:
      - "9090:9090"
    networks:
      bev_osint:
        ipv4_address: 172.21.0.24
    logging: *default-logging
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 2G

  # Grafana
  grafana:
    image: grafana/grafana:10.2.0
    platform: linux/amd64
    container_name: bev_grafana
    restart: always
    environment:
      GF_SERVER_HTTP_ADDR: 0.0.0.0
      GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD}
      GF_INSTALL_PLUGINS: grafana-clock-panel,grafana-simple-json-datasource,grafana-worldmap-panel
      GF_PATHS_PROVISIONING: /etc/grafana/provisioning
      GF_ANALYTICS_REPORTING_ENABLED: "false"
      GF_ANALYTICS_CHECK_FOR_UPDATES: "false"
      GF_LOG_LEVEL: warn
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
      - ./grafana/dashboards:/var/lib/grafana/dashboards:ro
      - logs:/var/log/grafana
    ports:
      - "3001:3000"
    networks:
      bev_osint:
        ipv4_address: 172.21.0.25
    logging: *default-logging
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 1G

  # Node Exporter
  node-exporter:
    image: prom/node-exporter:v1.6.1
    platform: linux/amd64
    container_name: bev_node_exporter
    restart: always
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    ports:
      - "9100:9100"
    networks:
      bev_osint:
        ipv4_address: 172.21.0.26
    logging: *default-logging
    deploy:
      resources:
        limits:
          memory: 256M

  # Airflow Scheduler
  airflow-scheduler:
    image: apache/airflow:2.7.2-python3.11
    platform: linux/amd64
    container_name: bev_airflow_scheduler
    restart: always
    depends_on:
      - postgres
      - redis
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/osint
      AIRFLOW__CELERY__RESULT_BACKEND: redis://:${REDIS_PASSWORD}@redis:6379/2
      AIRFLOW__CELERY__BROKER_URL: redis://:${REDIS_PASSWORD}@redis:6379/2
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth'
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: 'true'
      AIRFLOW__METRICS__STATSD_ON: 'true'
      AIRFLOW__METRICS__STATSD_HOST: prometheus
      AIRFLOW__METRICS__STATSD_PORT: 8125
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/config:/opt/airflow/config
      - logs:/var/log/airflow
    command: scheduler
    networks:
      bev_osint:
        ipv4_address: 172.21.0.27
    logging: *default-logging
    deploy:
      resources:
        limits:
          memory: 2G

  # Airflow Webserver
  airflow-webserver:
    image: apache/airflow:2.7.2-python3.11
    platform: linux/amd64
    container_name: bev_airflow_webserver
    restart: always
    depends_on:
      - postgres
      - redis
      - airflow-scheduler
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/osint
      AIRFLOW__CELERY__RESULT_BACKEND: redis://:${REDIS_PASSWORD}@redis:6379/2
      AIRFLOW__CELERY__BROKER_URL: redis://:${REDIS_PASSWORD}@redis:6379/2
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth'
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: 'true'
      AIRFLOW_ADMIN_USER: ${AIRFLOW_ADMIN_USER:-admin}
      AIRFLOW_ADMIN_PASSWORD: ${AIRFLOW_ADMIN_PASSWORD}
      AIRFLOW_ADMIN_EMAIL: ${AIRFLOW_ADMIN_EMAIL:-admin@bev.local}
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/config:/opt/airflow/config
      - logs:/var/log/airflow
    ports:
      - "8080:8080"
    command: webserver
    networks:
      bev_osint:
        ipv4_address: 172.21.0.28
    logging: *default-logging
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 2G

  # Airflow Worker 1
  airflow-worker-1:
    image: apache/airflow:2.7.2-python3.11
    platform: linux/amd64
    container_name: bev_airflow_worker_1
    restart: always
    depends_on:
      - postgres
      - redis
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/osint
      AIRFLOW__CELERY__RESULT_BACKEND: redis://:${REDIS_PASSWORD}@redis:6379/2
      AIRFLOW__CELERY__BROKER_URL: redis://:${REDIS_PASSWORD}@redis:6379/2
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
      AIRFLOW__CELERY__WORKER_CONCURRENCY: 4
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/config:/opt/airflow/config
      - logs:/var/log/airflow
    command: celery worker
    networks:
      bev_osint:
        ipv4_address: 172.21.0.29
    logging: *default-logging
    deploy:
      resources:
        limits:
          memory: 3G

  # Airflow Worker 2
  airflow-worker-2:
    image: apache/airflow:2.7.2-python3.11
    platform: linux/amd64
    container_name: bev_airflow_worker_2
    restart: always
    depends_on:
      - postgres
      - redis
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/osint
      AIRFLOW__CELERY__RESULT_BACKEND: redis://:${REDIS_PASSWORD}@redis:6379/2
      AIRFLOW__CELERY__BROKER_URL: redis://:${REDIS_PASSWORD}@redis:6379/2
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
      AIRFLOW__CELERY__WORKER_CONCURRENCY: 4
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/config:/opt/airflow/config
      - logs:/var/log/airflow
    command: celery worker
    networks:
      bev_osint:
        ipv4_address: 172.21.0.30
    logging: *default-logging
    deploy:
      resources:
        limits:
          memory: 3G

  # Airflow Worker 3
  airflow-worker-3:
    image: apache/airflow:2.7.2-python3.11
    platform: linux/amd64
    container_name: bev_airflow_worker_3
    restart: always
    depends_on:
      - postgres
      - redis
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/osint
      AIRFLOW__CELERY__RESULT_BACKEND: redis://:${REDIS_PASSWORD}@redis:6379/2
      AIRFLOW__CELERY__BROKER_URL: redis://:${REDIS_PASSWORD}@redis:6379/2
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
      AIRFLOW__CELERY__WORKER_CONCURRENCY: 4
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/config:/opt/airflow/config
      - logs:/var/log/airflow
    command: celery worker
    networks:
      bev_osint:
        ipv4_address: 172.21.0.31
    logging: *default-logging
    deploy:
      resources:
        limits:
          memory: 3G

  # ================================================
  # PHASE 2: Document Processing Layer
  # ================================================

  # OCR Service
  ocr-service:
    build:
      context: ./thanos/phase2/ocr
      dockerfile: Dockerfile
    container_name: bev_ocr_service
    restart: always
    depends_on:
      - redis
      - postgres
    environment:
      REDIS_URL: redis://:${REDIS_PASSWORD}@redis:6379/3
      POSTGRES_URI: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/osint
      OCR_WORKERS: 4
      TESSDATA_PREFIX: /usr/share/tesseract-ocr/5/tessdata
      MAX_FILE_SIZE: 50MB
      SUPPORTED_FORMATS: "pdf,png,jpg,jpeg,tiff,webp"
    volumes:
      - ocr_data:/app/data
      - ./thanos/phase2/ocr/tessdata:/usr/share/tesseract-ocr/5/tessdata:ro
      - logs:/var/log/ocr
    ports:
      - "8001:8000"
    networks:
      bev_osint:
        ipv4_address: 172.21.0.32
    logging: *default-logging
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 4G

  # Document Analyzer Worker 1
  doc-analyzer-1:
    build:
      context: ./thanos/phase2/analyzer
      dockerfile: Dockerfile
    container_name: bev_doc_analyzer_1
    runtime: nvidia
    restart: always
    depends_on:
      - redis
      - postgres
      - elasticsearch
      - neo4j
    environment:
      REDIS_URL: redis://:${REDIS_PASSWORD}@redis:6379/3
      POSTGRES_URI: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/osint
      ELASTICSEARCH_URL: http://elasticsearch:9200
      NEO4J_URI: ${NEO4J_URI}
      NEO4J_USER: ${NEO4J_USER}
      NEO4J_PASSWORD: ${NEO4J_PASSWORD}
      WORKER_ID: 1
      CONCURRENT_JOBS: 3
      NLP_MODEL: en_core_web_sm
      ENABLE_GPU: "true"
    volumes:
      - document_analyzer_data:/app/data
      - ./thanos/phase2/analyzer/models:/app/models:ro
      - logs:/var/log/analyzer
    networks:
      bev_osint:
        ipv4_address: 172.21.0.33
    logging: *default-logging
    deploy:
      resources:
        limits:
          memory: 6G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Document Analyzer Worker 2
  doc-analyzer-2:
    build:
      context: ./thanos/phase2/analyzer
      dockerfile: Dockerfile
    container_name: bev_doc_analyzer_2
    runtime: nvidia
    restart: always
    depends_on:
      - redis
      - postgres
      - elasticsearch
      - neo4j
    environment:
      REDIS_URL: redis://:${REDIS_PASSWORD}@redis:6379/3
      POSTGRES_URI: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/osint
      ELASTICSEARCH_URL: http://elasticsearch:9200
      NEO4J_URI: ${NEO4J_URI}
      NEO4J_USER: ${NEO4J_USER}
      NEO4J_PASSWORD: ${NEO4J_PASSWORD}
      WORKER_ID: 2
      CONCURRENT_JOBS: 3
      NLP_MODEL: en_core_web_sm
      ENABLE_GPU: "true"
    volumes:
      - document_analyzer_data:/app/data
      - ./thanos/phase2/analyzer/models:/app/models:ro
      - logs:/var/log/analyzer
    networks:
      bev_osint:
        ipv4_address: 172.21.0.34
    logging: *default-logging
    deploy:
      resources:
        limits:
          memory: 6G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Document Analyzer Worker 3
  doc-analyzer-3:
    build:
      context: ./thanos/phase2/analyzer
      dockerfile: Dockerfile
    container_name: bev_doc_analyzer_3
    runtime: nvidia
    restart: always
    depends_on:
      - redis
      - postgres
      - elasticsearch
      - neo4j
    environment:
      REDIS_URL: redis://:${REDIS_PASSWORD}@redis:6379/3
      POSTGRES_URI: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/osint
      ELASTICSEARCH_URL: http://elasticsearch:9200
      NEO4J_URI: ${NEO4J_URI}
      NEO4J_USER: ${NEO4J_USER}
      NEO4J_PASSWORD: ${NEO4J_PASSWORD}
      WORKER_ID: 3
      CONCURRENT_JOBS: 3
      NLP_MODEL: en_core_web_sm
      ENABLE_GPU: "true"
    volumes:
      - document_analyzer_data:/app/data
      - ./thanos/phase2/analyzer/models:/app/models:ro
      - logs:/var/log/analyzer
    networks:
      bev_osint:
        ipv4_address: 172.21.0.35
    logging: *default-logging
    deploy:
      resources:
        limits:
          memory: 6G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # ================================================
  # PHASE 3: Intelligence Swarm Layer
  # ================================================

  # Swarm Master 1
  swarm-master-1:
    build:
      context: ./thanos/phase3/swarm
      dockerfile: Dockerfile
    container_name: bev_swarm_master_1
    runtime: nvidia
    restart: always
    depends_on:
      - redis
      - postgres
      - neo4j
      - kafka-1
    environment:
      SWARM_ID: master-1
      REDIS_URL: redis://:${REDIS_PASSWORD}@redis:6379/4
      POSTGRES_URI: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/osint
      NEO4J_URI: ${NEO4J_URI}
      NEO4J_USER: ${NEO4J_USER}
      NEO4J_PASSWORD: ${NEO4J_PASSWORD}
      KAFKA_BROKERS: kafka-1:9092,kafka-2:9092,kafka-3:9092
      SWARM_ROLE: master
      MAX_AGENTS: 50
      COORDINATION_STRATEGY: democratic
      CONSENSUS_THRESHOLD: 0.7
    volumes:
      - swarm_data:/app/data
      - ./thanos/phase3/swarm/strategies:/app/strategies:ro
      - logs:/var/log/swarm
    ports:
      - "8002:8000"
    networks:
      bev_osint:
        ipv4_address: 172.21.0.36
    logging: *default-logging
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Swarm Master 2
  swarm-master-2:
    build:
      context: ./thanos/phase3/swarm
      dockerfile: Dockerfile
    container_name: bev_swarm_master_2
    runtime: nvidia
    restart: always
    depends_on:
      - redis
      - postgres
      - neo4j
      - kafka-1
    environment:
      SWARM_ID: master-2
      REDIS_URL: redis://:${REDIS_PASSWORD}@redis:6379/4
      POSTGRES_URI: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/osint
      NEO4J_URI: ${NEO4J_URI}
      NEO4J_USER: ${NEO4J_USER}
      NEO4J_PASSWORD: ${NEO4J_PASSWORD}
      KAFKA_BROKERS: kafka-1:9092,kafka-2:9092,kafka-3:9092
      SWARM_ROLE: master
      MAX_AGENTS: 50
      COORDINATION_STRATEGY: hierarchical
      CONSENSUS_THRESHOLD: 0.7
    volumes:
      - swarm_data:/app/data
      - ./thanos/phase3/swarm/strategies:/app/strategies:ro
      - logs:/var/log/swarm
    ports:
      - "8003:8000"
    networks:
      bev_osint:
        ipv4_address: 172.21.0.37
    logging: *default-logging
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Research Coordinator
  research-coordinator:
    build:
      context: ./thanos/phase3/coordinator
      dockerfile: Dockerfile
    container_name: bev_research_coordinator
    restart: always
    depends_on:
      - redis
      - postgres
      - neo4j
      - elasticsearch
    environment:
      REDIS_URL: redis://:${REDIS_PASSWORD}@redis:6379/5
      POSTGRES_URI: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/osint
      NEO4J_URI: ${NEO4J_URI}
      NEO4J_USER: ${NEO4J_USER}
      NEO4J_PASSWORD: ${NEO4J_PASSWORD}
      ELASTICSEARCH_URL: http://elasticsearch:9200
      RESEARCH_DEPTH: advanced
      PARALLEL_STREAMS: 8
      CORRELATION_THRESHOLD: 0.6
      AUTO_PIVOT: "true"
    volumes:
      - ./thanos/phase3/coordinator/research_templates:/app/templates:ro
      - logs:/var/log/coordinator
    ports:
      - "8004:8000"
    networks:
      bev_osint:
        ipv4_address: 172.21.0.38
    logging: *default-logging
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 3G

  # Memory Manager
  memory-manager:
    build:
      context: ./thanos/phase3/memory
      dockerfile: Dockerfile
    container_name: bev_memory_manager
    restart: always
    depends_on:
      - redis
      - postgres
      - neo4j
      - elasticsearch
    environment:
      REDIS_URL: redis://:${REDIS_PASSWORD}@redis:6379/6
      POSTGRES_URI: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/osint
      NEO4J_URI: ${NEO4J_URI}
      NEO4J_USER: ${NEO4J_USER}
      NEO4J_PASSWORD: ${NEO4J_PASSWORD}
      ELASTICSEARCH_URL: http://elasticsearch:9200
      MEMORY_RETENTION_DAYS: 365
      COMPRESSION_ENABLED: "true"
      VECTOR_DIMENSIONS: 1536
      SIMILARITY_THRESHOLD: 0.8
    volumes:
      - memory_data:/app/data
      - ./thanos/phase3/memory/embeddings:/app/embeddings:ro
      - logs:/var/log/memory
    ports:
      - "8005:8000"
    networks:
      bev_osint:
        ipv4_address: 172.21.0.39
    logging: *default-logging
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 8G

  # Code Optimizer
  code-optimizer:
    build:
      context: ./thanos/phase3/optimizer
      dockerfile: Dockerfile
    container_name: bev_code_optimizer
    restart: always
    depends_on:
      - redis
      - postgres
    environment:
      REDIS_URL: redis://:${REDIS_PASSWORD}@redis:6379/7
      POSTGRES_URI: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/osint
      OPTIMIZATION_LEVEL: aggressive
      PARALLEL_JOBS: 6
      AUTO_REFACTOR: "true"
      SAFETY_CHECKS: "true"
      BACKUP_ENABLED: "true"
    volumes:
      - code_data:/app/data
      - ./thanos/phase3/optimizer/templates:/app/templates:ro
      - /home/starlord/Bev/src:/app/source:rw
      - logs:/var/log/optimizer
    ports:
      - "8006:8000"
    networks:
      bev_osint:
        ipv4_address: 172.21.0.40
    logging: *default-logging
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 4G

  # Tool Coordinator
  tool-coordinator:
    build:
      context: ./thanos/phase3/tools
      dockerfile: Dockerfile
    container_name: bev_tool_coordinator
    restart: always
    depends_on:
      - redis
      - postgres
      - neo4j
    environment:
      REDIS_URL: redis://:${REDIS_PASSWORD}@redis:6379/8
      POSTGRES_URI: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/osint
      NEO4J_URI: ${NEO4J_URI}
      NEO4J_USER: ${NEO4J_USER}
      NEO4J_PASSWORD: ${NEO4J_PASSWORD}
      TOOL_REGISTRY_URL: http://0.0.0.0:8007/tools
      AUTO_DISCOVERY: "true"
      HEALTH_CHECK_INTERVAL: 60
      LOAD_BALANCING: round_robin
    volumes:
      - tool_data:/app/data
      - ./thanos/phase3/tools/registry:/app/registry:ro
      - logs:/var/log/tools
    ports:
      - "8007:8000"
    networks:
      bev_osint:
        ipv4_address: 172.21.0.41
    logging: *default-logging
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 2G

  # ================================================
  # PHASE 4: Security & Privacy Layer
  # ================================================

  # HashiCorp Vault
  vault:
    image: vault:1.15.2
    platform: linux/amd64
    container_name: bev_vault
    restart: always
    cap_add:
      - IPC_LOCK
    environment:
      VAULT_DEV_ROOT_TOKEN_ID: ${VAULT_ROOT_TOKEN}
      VAULT_DEV_LISTEN_ADDRESS: 0.0.0.0:8200
      VAULT_LOCAL_CONFIG: |
        {
          "storage": {
            "file": {
              "path": "/vault/data"
            }
          },
          "listener": {
            "tcp": {
              "address": "0.0.0.0:8200",
              "tls_disable": "true"
            }
          },
          "ui": "true",
          "api_addr": "http://0.0.0.0:8200"
        }
    volumes:
      - vault_data:/vault/data
      - ./thanos/phase4/vault/config:/vault/config:ro
      - logs:/vault/logs
    ports:
      - "8200:8200"
    networks:
      bev_osint:
        ipv4_address: 172.21.0.42
    logging: *default-logging
    healthcheck:
      test: ["CMD", "vault", "status"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 1G

  # Guardian Enforcer 1
  guardian-enforcer-1:
    build:
      context: ./thanos/phase4/guardian
      dockerfile: Dockerfile
    container_name: bev_guardian_1
    restart: always
    depends_on:
      - vault
      - redis
      - postgres
    environment:
      GUARDIAN_ID: enforcer-1
      VAULT_ADDR: http://vault:8200
      VAULT_TOKEN: ${VAULT_ROOT_TOKEN}
      REDIS_URL: redis://:${REDIS_PASSWORD}@redis:6379/9
      POSTGRES_URI: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/osint
      SECURITY_LEVEL: high
      MONITORING_MODE: active
      THREAT_RESPONSE: automatic
      INCIDENT_ESCALATION: "true"
    volumes:
      - guardian_data:/app/data
      - ./thanos/phase4/guardian/rules:/app/rules:ro
      - logs:/var/log/guardian
    ports:
      - "8008:8000"
    networks:
      bev_osint:
        ipv4_address: 172.21.0.43
    logging: *default-logging
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 2G

  # Guardian Enforcer 2
  guardian-enforcer-2:
    build:
      context: ./thanos/phase4/guardian
      dockerfile: Dockerfile
    container_name: bev_guardian_2
    restart: always
    depends_on:
      - vault
      - redis
      - postgres
    environment:
      GUARDIAN_ID: enforcer-2
      VAULT_ADDR: http://vault:8200
      VAULT_TOKEN: ${VAULT_ROOT_TOKEN}
      REDIS_URL: redis://:${REDIS_PASSWORD}@redis:6379/9
      POSTGRES_URI: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/osint
      SECURITY_LEVEL: high
      MONITORING_MODE: active
      THREAT_RESPONSE: automatic
      INCIDENT_ESCALATION: "true"
    volumes:
      - guardian_data:/app/data
      - ./thanos/phase4/guardian/rules:/app/rules:ro
      - logs:/var/log/guardian
    ports:
      - "8009:8000"
    networks:
      bev_osint:
        ipv4_address: 172.21.0.44
    logging: *default-logging
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 2G

  # Tor Node 1
  tor-node-1:
    image: torproject/tor:latest
    platform: linux/amd64
    container_name: bev_tor_node_1
    restart: always
    environment:
      TOR_NICKNAME: BevNode1
      TOR_RELAY_PORT: 9001
      TOR_DIR_PORT: 9030
      TOR_BANDWIDTH_RATE: 100MB
      TOR_BANDWIDTH_BURST: 200MB
    volumes:
      - tor_nodes_data:/var/lib/tor
      - ./thanos/phase4/tor/node1.conf:/etc/tor/torrc:ro
      - logs:/var/log/tor
    ports:
      - "9001:9001"
      - "9030:9030"
    networks:
      bev_osint:
        ipv4_address: 172.21.0.45
    logging: *default-logging
    deploy:
      resources:
        limits:
          memory: 512M

  # Tor Node 2
  tor-node-2:
    image: torproject/tor:latest
    platform: linux/amd64
    container_name: bev_tor_node_2
    restart: always
    environment:
      TOR_NICKNAME: BevNode2
      TOR_RELAY_PORT: 9002
      TOR_DIR_PORT: 9031
      TOR_BANDWIDTH_RATE: 100MB
      TOR_BANDWIDTH_BURST: 200MB
    volumes:
      - tor_nodes_data:/var/lib/tor
      - ./thanos/phase4/tor/node2.conf:/etc/tor/torrc:ro
      - logs:/var/log/tor
    ports:
      - "9002:9002"
      - "9031:9031"
    networks:
      bev_osint:
        ipv4_address: 172.21.0.46
    logging: *default-logging
    deploy:
      resources:
        limits:
          memory: 512M

  # Tor Node 3
  tor-node-3:
    image: torproject/tor:latest
    platform: linux/amd64
    container_name: bev_tor_node_3
    restart: always
    environment:
      TOR_NICKNAME: BevNode3
      TOR_RELAY_PORT: 9003
      TOR_DIR_PORT: 9032
      TOR_BANDWIDTH_RATE: 100MB
      TOR_BANDWIDTH_BURST: 200MB
    volumes:
      - tor_nodes_data:/var/lib/tor
      - ./thanos/phase4/tor/node3.conf:/etc/tor/torrc:ro
      - logs:/var/log/tor
    ports:
      - "9003:9003"
      - "9032:9032"
    networks:
      bev_osint:
        ipv4_address: 172.21.0.47
    logging: *default-logging
    deploy:
      resources:
        limits:
          memory: 512M

  # Intrusion Detection System
  ids:
    build:
      context: ./thanos/phase4/ids
      dockerfile: Dockerfile
    container_name: bev_ids
    restart: always
    depends_on:
      - redis
      - postgres
      - elasticsearch
    environment:
      REDIS_URL: redis://:${REDIS_PASSWORD}@redis:6379/10
      POSTGRES_URI: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/osint
      ELASTICSEARCH_URL: http://elasticsearch:9200
      IDS_MODE: active
      ALERT_THRESHOLD: medium
      AUTO_BLOCK: "true"
      WHITELIST_ENABLED: "true"
      ML_DETECTION: "true"
    volumes:
      - ids_data:/app/data
      - ./thanos/phase4/ids/rules:/app/rules:ro
      - security_logs:/var/log/ids
    ports:
      - "8010:8000"
    networks:
      bev_osint:
        ipv4_address: 172.21.0.48
    logging: *default-logging
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 4G

  # Traffic Analyzer
  traffic-analyzer:
    build:
      context: ./thanos/phase4/traffic
      dockerfile: Dockerfile
    container_name: bev_traffic_analyzer
    restart: always
    depends_on:
      - redis
      - postgres
      - influxdb
    environment:
      REDIS_URL: redis://:${REDIS_PASSWORD}@redis:6379/11
      POSTGRES_URI: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/osint
      INFLUXDB_URL: http://influxdb:8086
      INFLUXDB_TOKEN: ${INFLUXDB_TOKEN}
      ANALYSIS_MODE: realtime
      PACKET_CAPTURE: "true"
      DPI_ENABLED: "true"
      GEOLOCATION: "true"
    volumes:
      - ./thanos/phase4/traffic/captures:/app/captures
      - security_logs:/var/log/traffic
    ports:
      - "8011:8000"
    networks:
      bev_osint:
        ipv4_address: 172.21.0.49
    logging: *default-logging
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 6G

  # Anomaly Detector
  anomaly-detector:
    build:
      context: ./thanos/phase4/anomaly
      dockerfile: Dockerfile
    container_name: bev_anomaly_detector
    restart: always
    depends_on:
      - redis
      - postgres
      - elasticsearch
      - influxdb
    environment:
      REDIS_URL: redis://:${REDIS_PASSWORD}@redis:6379/12
      POSTGRES_URI: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/osint
      ELASTICSEARCH_URL: http://elasticsearch:9200
      INFLUXDB_URL: http://influxdb:8086
      INFLUXDB_TOKEN: ${INFLUXDB_TOKEN}
      ML_MODEL: isolation_forest
      SENSITIVITY: high
      REAL_TIME: "true"
      AUTO_LEARNING: "true"
    volumes:
      - ./thanos/phase4/anomaly/models:/app/models
      - security_logs:/var/log/anomaly
    ports:
      - "8012:8000"
    networks:
      bev_osint:
        ipv4_address: 172.21.0.50
    logging: *default-logging
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 8G

  # ================================================
  # PHASE 5: Autonomous Interface Layer
  # ================================================

  # Autonomous Controller 1
  autonomous-controller-1:
    build:
      context: ./thanos/phase5/controller
      dockerfile: Dockerfile
    container_name: bev_autonomous_1
    restart: always
    depends_on:
      - redis
      - postgres
      - neo4j
      - vault
    environment:
      CONTROLLER_ID: autonomous-1
      REDIS_URL: redis://:${REDIS_PASSWORD}@redis:6379/13
      POSTGRES_URI: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/osint
      NEO4J_URI: ${NEO4J_URI}
      NEO4J_USER: ${NEO4J_USER}
      NEO4J_PASSWORD: ${NEO4J_PASSWORD}
      VAULT_ADDR: http://vault:8200
      VAULT_TOKEN: ${VAULT_ROOT_TOKEN}
      AUTONOMY_LEVEL: supervised
      DECISION_CONFIDENCE: 0.85
      LEARNING_RATE: adaptive
      SAFETY_OVERRIDE: "true"
    volumes:
      - autonomous_data:/app/data
      - ./thanos/phase5/controller/policies:/app/policies:ro
      - logs:/var/log/autonomous
    ports:
      - "8013:8000"
    networks:
      bev_osint:
        ipv4_address: 172.21.0.51
    logging: *default-logging
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 6G

  # Autonomous Controller 2
  autonomous-controller-2:
    build:
      context: ./thanos/phase5/controller
      dockerfile: Dockerfile
    container_name: bev_autonomous_2
    restart: always
    depends_on:
      - redis
      - postgres
      - neo4j
      - vault
    environment:
      CONTROLLER_ID: autonomous-2
      REDIS_URL: redis://:${REDIS_PASSWORD}@redis:6379/13
      POSTGRES_URI: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/osint
      NEO4J_URI: ${NEO4J_URI}
      NEO4J_USER: ${NEO4J_USER}
      NEO4J_PASSWORD: ${NEO4J_PASSWORD}
      VAULT_ADDR: http://vault:8200
      VAULT_TOKEN: ${VAULT_ROOT_TOKEN}
      AUTONOMY_LEVEL: supervised
      DECISION_CONFIDENCE: 0.85
      LEARNING_RATE: adaptive
      SAFETY_OVERRIDE: "true"
    volumes:
      - autonomous_data:/app/data
      - ./thanos/phase5/controller/policies:/app/policies:ro
      - logs:/var/log/autonomous
    ports:
      - "8014:8000"
    networks:
      bev_osint:
        ipv4_address: 172.21.0.52
    logging: *default-logging
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 6G

  # Live2D Avatar Backend
  live2d-avatar:
    build:
      context: ./thanos/phase5/live2d/backend
      dockerfile: Dockerfile
    container_name: bev_live2d_avatar
    restart: always
    depends_on:
      - redis
      - postgres
    environment:
      REDIS_URL: redis://:${REDIS_PASSWORD}@redis:6379/14
      POSTGRES_URI: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/osint
      AVATAR_MODEL: bev_model_v1
      VOICE_ENGINE: espeak
      TTS_VOICE: en-us
      EMOTION_ENGINE: facial_expressions
      GESTURE_SYNC: "true"
      REAL_TIME_RESPONSE: "true"
    volumes:
      - live2d_data:/app/data
      - avatar_assets:/app/assets
      - ./thanos/phase5/live2d/models:/app/models:ro
      - logs:/var/log/live2d
    ports:
      - "8015:8000"
      - "9001:9001"  # WebSocket
    networks:
      bev_osint:
        ipv4_address: 172.21.0.53
    logging: *default-logging
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 4G

  # Live2D Frontend
  live2d-frontend:
    build:
      context: ./thanos/phase5/live2d/frontend
      dockerfile: Dockerfile
    container_name: bev_live2d_frontend
    restart: always
    depends_on:
      - live2d-avatar
    environment:
      NODE_ENV: production
      REACT_APP_AVATAR_API: http://172.21.0.53:8000
      REACT_APP_WEBSOCKET_URL: ws://172.21.0.53:9001
      REACT_APP_TITLE: "Bev - OSINT Intelligence Avatar"
      GENERATE_SOURCEMAP: "false"
    volumes:
      - ./thanos/phase5/live2d/frontend/public:/app/public
      - logs:/var/log/frontend
    ports:
      - "3002:3000"
    networks:
      bev_osint:
        ipv4_address: 172.21.0.54
    logging: *default-logging
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 1G
