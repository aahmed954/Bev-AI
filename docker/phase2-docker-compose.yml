version: '3.8'

# Phase 2 Complete OCR and Document Processing Pipeline
# All services ARM-compatible and production-ready

networks:
  oracle1-net:
    driver: bridge
    ipam:
      config:
        - subnet: 172.21.0.0/16

volumes:
  redis-data:
    driver: local
  rabbitmq-data:
    driver: local
  neo4j-data:
    driver: local
  neo4j-logs:
    driver: local
  minio-data1:
    driver: local
  minio-data2:
    driver: local
  minio-data3:
    driver: local

services:
  # Redis for Celery backend
  redis:
    image: redis:7-alpine
    container_name: oracle1_redis
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes --maxmemory 1gb --maxmemory-policy allkeys-lru
    networks:
      oracle1-net:
        ipv4_address: 172.21.0.10
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # RabbitMQ for message queuing
  rabbitmq:
    image: rabbitmq:3-management-alpine
    container_name: oracle1_rabbitmq
    restart: unless-stopped
    ports:
      - "5672:5672"
      - "15672:15672"
    environment:
      RABBITMQ_DEFAULT_USER: admin
      RABBITMQ_DEFAULT_PASS: admin123
    volumes:
      - rabbitmq-data:/var/lib/rabbitmq
    networks:
      oracle1-net:
        ipv4_address: 172.21.0.11
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "check_port_connectivity"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Neo4j for graph database
  neo4j:
    image: neo4j:5-community
    container_name: oracle1_neo4j
    restart: unless-stopped
    ports:
      - "7474:7474"
      - "7687:7687"
    environment:
      NEO4J_AUTH: neo4j/admin123
      NEO4J_PLUGINS: '["apoc"]'
      NEO4J_dbms_security_procedures_unrestricted: apoc.*
      NEO4J_dbms_memory_heap_initial__size: 1G
      NEO4J_dbms_memory_heap_max__size: 2G
    volumes:
      - neo4j-data:/data
      - neo4j-logs:/logs
    networks:
      oracle1-net:
        ipv4_address: 172.21.0.12
    healthcheck:
      test: ["CMD", "cypher-shell", "-u", "neo4j", "-p", "admin123", "RETURN 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  # OCR Service
  ocr-service:
    build:
      context: ./ocr-service
      dockerfile: Dockerfile
    container_name: oracle1_ocr_service
    restart: unless-stopped
    ports:
      - "8080:8080"
    environment:
      OCR_RABBITMQ_HOST: rabbitmq
      OCR_RABBITMQ_USER: admin
      OCR_RABBITMQ_PASSWORD: admin123
      OCR_DEBUG: "false"
    volumes:
      - /tmp/ocr-uploads:/app/uploads
      - /tmp/ocr-processed:/app/processed
    networks:
      oracle1-net:
        ipv4_address: 172.21.0.20
    depends_on:
      rabbitmq:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G

  # Document Analyzer
  document-analyzer:
    build:
      context: ./document-analyzer
      dockerfile: Dockerfile
    container_name: oracle1_document_analyzer
    restart: unless-stopped
    ports:
      - "8081:8081"
    environment:
      ANALYZER_NEO4J_URI: bolt://neo4j:7687
      ANALYZER_NEO4J_USER: neo4j
      ANALYZER_NEO4J_PASSWORD: admin123
      ANALYZER_RABBITMQ_HOST: rabbitmq
      ANALYZER_RABBITMQ_USER: admin
      ANALYZER_RABBITMQ_PASSWORD: admin123
      ANALYZER_DEBUG: "false"
    networks:
      oracle1-net:
        ipv4_address: 172.21.0.21
    depends_on:
      rabbitmq:
        condition: service_healthy
      neo4j:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 8G
        reservations:
          cpus: '2.0'
          memory: 4G

  # Celery Edge Worker
  celery-edge-worker:
    build:
      context: ./celery-pipeline
      dockerfile: Dockerfile
    container_name: oracle1_celery_edge_worker
    restart: unless-stopped
    command: celery -A celery_app worker -Q edge_computing --loglevel=info --concurrency=4
    environment:
      CELERY_REDIS_HOST: redis
      CELERY_REDIS_PORT: 6379
    networks:
      oracle1-net:
        ipv4_address: 172.21.0.30
    depends_on:
      redis:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G

  # Celery Genetic Worker
  celery-genetic-worker:
    build:
      context: ./celery-pipeline
      dockerfile: Dockerfile
    container_name: oracle1_celery_genetic_worker
    restart: unless-stopped
    command: celery -A celery_app worker -Q genetic_optimization --loglevel=info --concurrency=2
    environment:
      CELERY_REDIS_HOST: redis
      CELERY_REDIS_PORT: 6379
    networks:
      oracle1-net:
        ipv4_address: 172.21.0.31
    depends_on:
      redis:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 8G
        reservations:
          cpus: '2.0'
          memory: 4G

  # Celery Knowledge Worker
  celery-knowledge-worker:
    build:
      context: ./celery-pipeline
      dockerfile: Dockerfile
    container_name: oracle1_celery_knowledge_worker
    restart: unless-stopped
    command: celery -A celery_app worker -Q knowledge_synthesis --loglevel=info --concurrency=3
    environment:
      CELERY_REDIS_HOST: redis
      CELERY_REDIS_PORT: 6379
    networks:
      oracle1-net:
        ipv4_address: 172.21.0.32
    depends_on:
      redis:
        condition: service_healthy
      neo4j:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: '3.0'
          memory: 6G
        reservations:
          cpus: '1.5'
          memory: 3G

  # Celery ToolMaster Worker
  celery-toolmaster-worker:
    build:
      context: ./celery-pipeline
      dockerfile: Dockerfile
    container_name: oracle1_celery_toolmaster_worker
    restart: unless-stopped
    command: celery -A celery_app worker -Q tool_orchestration --loglevel=info --concurrency=2
    environment:
      CELERY_REDIS_HOST: redis
      CELERY_REDIS_PORT: 6379
    networks:
      oracle1-net:
        ipv4_address: 172.21.0.33
    depends_on:
      redis:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G

  # Celery Flower for monitoring
  celery-flower:
    build:
      context: ./celery-pipeline
      dockerfile: Dockerfile
    container_name: oracle1_celery_flower
    restart: unless-stopped
    ports:
      - "5555:5555"
    command: celery -A celery_app flower --loglevel=info
    environment:
      CELERY_REDIS_HOST: redis
      CELERY_REDIS_PORT: 6379
    networks:
      oracle1-net:
        ipv4_address: 172.21.0.34
    depends_on:
      redis:
        condition: service_healthy

  # MinIO Node 1
  minio1:
    image: minio/minio:latest
    hostname: minio1
    container_name: oracle1_minio1
    restart: unless-stopped
    ports:
      - "9001:9000"
      - "9011:9001"
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: admin123456
    command: server --console-address ":9001" http://minio{1...3}/data
    volumes:
      - minio-data1:/data
    networks:
      oracle1-net:
        ipv4_address: 172.21.0.40
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G

  # MinIO Node 2
  minio2:
    image: minio/minio:latest
    hostname: minio2
    container_name: oracle1_minio2
    restart: unless-stopped
    ports:
      - "9002:9000"
      - "9012:9001"
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: admin123456
    command: server --console-address ":9001" http://minio{1...3}/data
    volumes:
      - minio-data2:/data
    networks:
      oracle1-net:
        ipv4_address: 172.21.0.41
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G

  # MinIO Node 3
  minio3:
    image: minio/minio:latest
    hostname: minio3
    container_name: oracle1_minio3
    restart: unless-stopped
    ports:
      - "9003:9000"
      - "9013:9001"
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: admin123456
    command: server --console-address ":9001" http://minio{1...3}/data
    volumes:
      - minio-data3:/data
    networks:
      oracle1-net:
        ipv4_address: 172.21.0.42
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G

  # MinIO Load Balancer
  minio-lb:
    image: nginx:alpine
    hostname: minio-lb
    container_name: oracle1_minio_lb
    restart: unless-stopped
    ports:
      - "9000:80"
    volumes:
      - ./minio-cluster/nginx.conf:/etc/nginx/nginx.conf:ro
    networks:
      oracle1-net:
        ipv4_address: 172.21.0.43
    depends_on:
      - minio1
      - minio2
      - minio3
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3