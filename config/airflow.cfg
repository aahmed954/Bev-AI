# Apache Airflow Configuration for Bev Orchestration

[core]
# The folder where airflow pipelines live
dags_folder = /home/starlord/Bev/dags

# The folder where airflow should store its log files
base_log_folder = /home/starlord/Bev/logs

# Airflow can store logs remotely in AWS S3, Google Cloud Storage or Elastic Search.
remote_logging = False

# The SqlAlchemy connection string to the metadata database.
sql_alchemy_conn = postgresql+psycopg2://researcher:secure_research_pass_2024@localhost/airflow_db

# The executor class that airflow should use
executor = CeleryExecutor

# The amount of parallelism for the backfill
parallelism = 32

# The number of task instances allowed to run concurrently
dag_concurrency = 16

# Are DAGs paused by default at creation
dags_are_paused_at_creation = False

# The maximum number of active runs per DAG
max_active_runs_per_dag = 16

# Whether to load the examples that ship with Airflow
load_examples = False

# Where your Airflow plugins are stored
plugins_folder = /home/starlord/Bev/plugins

# Secret key to save connection passwords in the db
fernet_key = {{ AIRFLOW_FERNET_KEY }}

# Whether to disable pickling dags
donot_pickle = False

# How long to wait before timing out a dagbag import
dagbag_import_timeout = 120

[database]
# The SqlAlchemy connection string to the metadata database
sql_alchemy_conn = postgresql+psycopg2://researcher:secure_research_pass_2024@localhost/airflow_db
sql_alchemy_pool_size = 5
sql_alchemy_pool_recycle = 1800
sql_alchemy_max_overflow = 10

[celery]
# The Celery broker URL
broker_url = redis://localhost:6379/1

# The Celery result backend
result_backend = db+postgresql://researcher:secure_research_pass_2024@localhost/airflow_db

# Celery configuration
worker_concurrency = 16
worker_prefetch_multiplier = 1

[celery_broker_transport_options]
visibility_timeout = 21600

[operators]
# The default owner assigned to each new operator
default_owner = bev_system
default_cpus = 1
default_ram = 512
default_disk = 512
default_gpus = 0

[webserver]
# The base url of your Airflow installation
base_url = http://thanos:8080

# The ip specified when starting the web server
web_server_host = 0.0.0.0

# The port on which to run the web server
web_server_port = 8080

# Paths to the SSL certificate and key for the web server
web_server_ssl_cert =
web_server_ssl_key =

# Number of workers to run the Gunicorn web server
workers = 4

# The worker class to use for Gunicorn
worker_class = sync

# Log files for the web server
access_logfile = -
error_logfile = -

# Expose the configuration file in the web server
expose_config = True

# Set to true to turn on authentication
authenticate = True
auth_backend = airflow.contrib.auth.backends.password_auth

# Filter the list of dags by owner name
filter_by_owner = False

[email]
email_backend = airflow.utils.email.send_email_smtp

[smtp]
smtp_host = localhost
smtp_starttls = True
smtp_ssl = False
smtp_port = 25
smtp_mail_from = airflow@bev.system

[scheduler]
# Task instances listen for external kill signal
job_heartbeat_sec = 5

# The scheduler constantly tries to trigger new tasks
scheduler_heartbeat_sec = 5

# The number of seconds to wait between consecutive DAG file processing
min_file_process_interval = 30

# How often to scan the DAGs directory for new files
dag_dir_list_interval = 300

# How often to print the list of known DAGs
print_stats_interval = 30

# Local task jobs periodically heartbeat to the DB
scheduler_zombie_task_threshold = 300

# Turn off scheduler catchup by setting this to False
catchup_by_default = False

# Setting to True will make first task instance of a task
# ignore depends_on_past dependencies
ignore_first_depends_on_past = True

# Number of processes to use when processing DAG files
max_threads = 2

# Statsd settings
statsd_on = True
statsd_host = localhost
statsd_port = 8125
statsd_prefix = airflow

[ldap]
# LDAP server configuration (if using LDAP auth)
uri = 
user_filter = 
user_name_attr = 
group_member_attr = 
superuser_filter = 
data_profiler_filter = 
bind_user = 
bind_password = 
basedn = 
cacert = 
search_scope = 

[kerberos]
ccache = /tmp/airflow_krb5_ccache
principal = airflow
reinit_frequency = 3600
kinit_path = kinit
keytab = airflow.keytab

[cli]
api_client = airflow.api.client.local_client
endpoint_url = http://localhost:8080

[api]
auth_backend = airflow.api.auth.backend.default

[lineage]
backend = 

[atlas]
sasl_enabled = False
host = 
port = 21000
username = 
password = 

[hive]
default_hive_mapred_queue = 

[github_enterprise]
api_rev = v3

[admin]
# UI admin settings
hide_sensitive_variable_fields = True

# Custom Bev Configuration
[bev]
# Agent configuration
agent_timeout = 300
max_agent_retries = 3
agent_health_check_interval = 60

# Research settings
max_concurrent_research_tasks = 10
research_cache_ttl = 3600

# Security settings
enable_encryption = True
audit_logging = True

# Performance settings
enable_gpu_acceleration = True
memory_limit_per_task = 4096
cpu_limit_per_task = 2

# Integration settings
elasticsearch_host = localhost:9200
neo4j_uri = bolt://localhost:7687
redis_host = localhost:6379
