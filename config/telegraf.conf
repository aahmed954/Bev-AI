# Telegraf Configuration for BEV Infrastructure
# Multi-source metrics collection from THANOS node forwarding to InfluxDB on ORACLE1

# =============================================================================
# GLOBAL AGENT CONFIGURATION
# =============================================================================
[agent]
  # Data collection interval
  interval = "10s"

  # Rounds collection interval to 'interval'
  round_interval = true

  # Telegraf will send metrics to outputs in batches of at most
  # metric_batch_size metrics.
  metric_batch_size = 1000

  # Maximum number of unwritten metrics per output
  metric_buffer_limit = 10000

  # Collection jitter is used to jitter the collection by a random amount.
  collection_jitter = "0s"

  # Default flushing interval for all outputs
  flush_interval = "10s"

  # Jitter the flush interval by a random amount
  flush_jitter = "0s"

  # By default, precision will be set to the same timestamp order as the
  # collection interval
  precision = ""

  # Run telegraf with debug log messages
  debug = false

  # Run telegraf in quiet mode (error log messages only)
  quiet = false

  # Log only error level messages
  logtarget = "file"
  logfile = "/var/log/telegraf/telegraf.log"
  logfile_rotation_interval = "24h"
  logfile_rotation_max_size = "100MB"
  logfile_rotation_max_archives = 7

  # Override default hostname, if empty use os.Hostname()
  hostname = "${TELEGRAF_HOSTNAME:-thanos}"

  # If set to true, do no set the "host" tag in the telegraf agent
  omit_hostname = false

# =============================================================================
# OUTPUT PLUGINS
# =============================================================================

# Configuration for sending metrics to InfluxDB on ORACLE1
[[outputs.influxdb]]
  # The full HTTP or UDP URL for your InfluxDB instance
  urls = ["${INFLUXDB_URL:-http://oracle1:8086}"]

  # The target database for metrics
  database = "${INFLUXDB_DATABASE:-bev_metrics}"

  # The value of this tag will be used to determine the database
  database_tag = ""

  # Exclude the database tag from metrics
  exclude_database_tag = false

  # HTTP Basic Auth credentials
  username = "${INFLUXDB_USERNAME:-bev_user}"
  password = "${INFLUXDB_PASSWORD:-}"

  # HTTP User-Agent
  user_agent = "telegraf"

  # UDP payload size is the maximum packet size to send
  udp_payload = "512B"

  # Optional TLS Config for HTTPS
  tls_ca = "/etc/ssl/certs/ca-certificates.crt"
  tls_cert = "/etc/ssl/certs/telegraf.pem"
  tls_key = "/etc/ssl/private/telegraf.key"
  insecure_skip_verify = true

  # HTTP Timeout
  timeout = "30s"

  # Additional HTTP headers
  http_headers = {"X-Telegraf-Host" = "${TELEGRAF_HOSTNAME:-thanos}"}

  # Content-Encoding for write request body
  content_encoding = "gzip"

  # Skip database creation
  skip_database_creation = false

# Optional: Send metrics to Prometheus for scraping
[[outputs.prometheus_client]]
  # Address to listen on
  listen = ":${TELEGRAF_PROMETHEUS_PORT:-9273}"

  # Path to publish the metrics on
  path = "/metrics"

  # Expiration interval for each metric
  expiration_interval = "60s"

  # Collectors to enable
  collectors_exclude = ["gocollector", "process"]

  # Optional string as metric name prefix
  metric_version = 2

  # Optional TLS Config
  tls_cert = "/etc/ssl/certs/telegraf.pem"
  tls_key = "/etc/ssl/private/telegraf.key"

# =============================================================================
# INPUT PLUGINS
# =============================================================================

# System-level metrics using the cpu plugin
[[inputs.cpu]]
  # Whether to report per-cpu stats or not
  percpu = true

  # Whether to report total system cpu stats or not
  totalcpu = true

  # If true, collect raw CPU time metrics
  collect_cpu_time = false

  # If true, compute and report the sum of all non-idle CPU states
  report_active = false

# Memory usage statistics
[[inputs.mem]]
  # No additional configuration required

# Disk usage and IO statistics
[[inputs.disk]]
  # Mountpoints to ignore
  ignore_fs = ["tmpfs", "devtmpfs", "devfs", "iso9660", "overlay", "aufs", "squashfs"]

# Disk IO statistics
[[inputs.diskio]]
  # Devices to monitor (empty means all)
  devices = []

  # Skip serial number reporting
  skip_serial_number = false

# Network interface statistics
[[inputs.net]]
  # Interfaces to ignore
  ignore_protocol_stats = false

# Network socket statistics
[[inputs.netstat]]
  # No additional configuration required

# System processes statistics
[[inputs.processes]]
  # No additional configuration required

# System uptime
[[inputs.system]]
  # No additional configuration required

# Kernel statistics
[[inputs.kernel]]
  # No additional configuration required

# Kernel virtual memory statistics
[[inputs.kernel_vmstat]]
  # No additional configuration required

# Linux sysstat metrics
[[inputs.linux_sysctl_fs]]
  # No additional configuration required

# =============================================================================
# DOCKER MONITORING
# =============================================================================

# Docker container metrics
[[inputs.docker]]
  # Docker Endpoint
  endpoint = "unix:///var/run/docker.sock"

  # Containers to include and exclude
  gather_services = false
  container_names = []
  container_name_include = []
  container_name_exclude = []

  # Container states to include
  container_state_include = ["created", "restarting", "running", "removing", "paused", "exited", "dead"]
  container_state_exclude = []

  # Timeout for docker list, info, and stats commands
  timeout = "5s"

  # Whether to report for each container per-device blkio
  perdevice = true

  # Whether to report for each container total blkio
  total = false

  # Docker label keys to include
  docker_label_include = []
  docker_label_exclude = []

  # Tag environment variables
  tag_env = ["JAVA_HOME", "HEAP_SIZE"]

# =============================================================================
# DATABASE MONITORING
# =============================================================================

# PostgreSQL metrics
[[inputs.postgresql]]
  # Specify address via a url matching:
  address = "host=${POSTGRESQL_HOST:-localhost} user=${POSTGRESQL_USER:-telegraf} dbname=${POSTGRESQL_DB:-postgres} sslmode=disable"

  # Store address as tag
  outputaddress = "db01"

  # Connection max idle time
  max_lifetime = "0s"

  # A custom name for the database that will be used as the "server" tag
  # in the measurement output
  databases = ["postgres", "bev"]

# Redis metrics
[[inputs.redis]]
  # Specify servers via a url matching:
  servers = ["tcp://${REDIS_HOST:-localhost}:${REDIS_PORT:-6379}"]

  # Redis AUTH password
  password = "${REDIS_PASSWORD:-}"

  # Optional TLS Config
  tls_ca = "/etc/ssl/certs/ca-certificates.crt"
  tls_cert = "/etc/ssl/certs/telegraf.pem"
  tls_key = "/etc/ssl/private/telegraf.key"
  insecure_skip_verify = true

# MongoDB metrics
[[inputs.mongodb]]
  # Array of MongoDB connection strings
  servers = ["mongodb://${MONGODB_HOST:-localhost}:${MONGODB_PORT:-27017}/admin"]

  # When true, collect cluster status
  gather_cluster_status = true

  # When true, collect per database status
  gather_perdb_stats = true

  # When true, collect per collection status
  gather_col_stats = true

  # When true, collect usage statistics for each collection
  gather_top_stat = true

# Elasticsearch metrics
[[inputs.elasticsearch]]
  # Elasticsearch servers
  servers = ["http://${ELASTICSEARCH_HOST:-localhost}:${ELASTICSEARCH_PORT:-9200}"]

  # HTTP Basic Authentication username and password
  username = "${ELASTICSEARCH_USERNAME:-}"
  password = "${ELASTICSEARCH_PASSWORD:-}"

  # Enable cluster health metrics
  cluster_health = true

  # Enable cluster stats metrics
  cluster_stats = true

  # Enable indices stats metrics
  indices_stats = ["primaries", "total"]

  # Enable node stats metrics
  node_stats = ["jvm", "os", "process", "thread_pool", "transport", "breaker"]

  # HTTP timeout
  http_timeout = "5s"

  # Optional TLS Config
  tls_ca = "/etc/ssl/certs/ca-certificates.crt"
  insecure_skip_verify = true

# =============================================================================
# APPLICATION MONITORING
# =============================================================================

# Nginx metrics via status module
[[inputs.nginx]]
  # An array of Nginx status URLs
  urls = ["http://${NGINX_HOST:-localhost}/nginx_status"]

  # HTTP timeout
  response_timeout = "5s"

  # Optional TLS Config
  tls_ca = "/etc/ssl/certs/ca-certificates.crt"
  insecure_skip_verify = true

# Apache HTTP Server metrics
[[inputs.apache]]
  # Array of URLs to gather from
  urls = ["http://${APACHE_HOST:-localhost}/server-status?auto"]

  # Credentials for basic HTTP authentication
  username = "${APACHE_USERNAME:-}"
  password = "${APACHE_PASSWORD:-}"

  # HTTP timeout
  response_timeout = "5s"

# HAProxy metrics
[[inputs.haproxy]]
  # Servers to monitor
  servers = ["http://${HAPROXY_HOST:-localhost}/haproxy?stats;csv"]

  # HTTP timeout
  timeout = "5s"

  # Optional HTTP Basic Auth Credentials
  username = "${HAPROXY_USERNAME:-}"
  password = "${HAPROXY_PASSWORD:-}"

# =============================================================================
# PROMETHEUS METRICS SCRAPING
# =============================================================================

# Scrape Prometheus metrics from various exporters
[[inputs.prometheus]]
  # URLs to scrape metrics from
  urls = [
    "http://${PROMETHEUS_HOST:-localhost}:${PROMETHEUS_PORT:-9090}/metrics",
    "http://${NODE_EXPORTER_HOST:-localhost}:${NODE_EXPORTER_PORT:-9100}/metrics",
    "http://${CADVISOR_HOST:-localhost}:${CADVISOR_PORT:-8080}/metrics",
    "http://${BLACKBOX_EXPORTER_HOST:-localhost}:${BLACKBOX_EXPORTER_PORT:-9115}/metrics"
  ]

  # Bearer token for authentication
  bearer_token = "${PROMETHEUS_BEARER_TOKEN:-}"

  # HTTP timeout
  response_timeout = "30s"

  # Metric version
  metric_version = 2

  # Optional TLS Config
  tls_ca = "/etc/ssl/certs/ca-certificates.crt"
  insecure_skip_verify = true

# Scrape Thanos component metrics
[[inputs.prometheus]]
  # Thanos component URLs
  urls = [
    "http://${THANOS_SIDECAR_HOST:-localhost}:${THANOS_SIDECAR_PORT:-19191}/metrics",
    "http://${THANOS_QUERY_HOST:-localhost}:${THANOS_QUERY_PORT:-19192}/metrics",
    "http://${THANOS_STORE_HOST:-localhost}:${THANOS_STORE_PORT:-19191}/metrics",
    "http://${THANOS_COMPACTOR_HOST:-localhost}:${THANOS_COMPACTOR_PORT:-19191}/metrics",
    "http://${THANOS_RECEIVER_HOST:-localhost}:${THANOS_RECEIVER_PORT:-19291}/metrics"
  ]

  # Add thanos tag to all metrics
  [inputs.prometheus.tags]
    component = "thanos"

  # Bearer token for authentication
  bearer_token = "${THANOS_BEARER_TOKEN:-}"

  # HTTP timeout
  response_timeout = "30s"

  # Metric version
  metric_version = 2

# Scrape Airflow metrics
[[inputs.prometheus]]
  # Airflow URLs
  urls = [
    "http://${AIRFLOW_WEBSERVER_HOST:-localhost}:${AIRFLOW_WEBSERVER_PORT:-8080}/admin/metrics",
    "http://${AIRFLOW_FLOWER_HOST:-localhost}:${AIRFLOW_FLOWER_PORT:-5555}/metrics"
  ]

  # Add airflow tag to all metrics
  [inputs.prometheus.tags]
    component = "airflow"

  # Bearer token for authentication
  bearer_token = "${AIRFLOW_BEARER_TOKEN:-}"

  # HTTP timeout
  response_timeout = "30s"

# =============================================================================
# LOG MONITORING
# =============================================================================

# Monitor system logs
[[inputs.logparser]]
  # Files to tail
  files = ["/var/log/syslog", "/var/log/messages", "/var/log/kern.log"]

  # Read from beginning of file
  from_beginning = false

  # Watch method
  watch_method = "inotify"

  # Grok patterns for parsing
  [inputs.logparser.grok]
    patterns = ["%{SYSLOGTIMESTAMP:timestamp} %{WORD:hostname} %{WORD:program}: %{GREEDYDATA:message}"]
    measurement = "syslog"
    timezone = "UTC"

# =============================================================================
# CUSTOM METRICS
# =============================================================================

# Custom BEV agent metrics (if available)
[[inputs.http]]
  # BEV agent endpoints
  urls = [
    "http://${BEV_AGENT_HOST:-localhost}:8001/metrics",
    "http://${BEV_AGENT_HOST:-localhost}:8002/metrics",
    "http://${BEV_AGENT_HOST:-localhost}:8003/metrics",
    "http://${BEV_AGENT_HOST:-localhost}:8004/metrics"
  ]

  # Data format
  data_format = "prometheus"

  # Add custom tags
  [inputs.http.tags]
    source = "bev_agents"

  # HTTP timeout
  timeout = "10s"

  # Ignore HTTP errors
  ignore_http_status_codes = [404, 503]

# =============================================================================
# PROCESSOR PLUGINS
# =============================================================================

# Add hostname tag to all metrics
[[processors.enum]]
  [[processors.enum.mapping]]
    tag = "host"
    dest = "hostname"
    [processors.enum.mapping.value_mappings]
      "localhost" = "${TELEGRAF_HOSTNAME:-thanos}"

# Convert string fields to tags for better indexing
[[processors.strings]]
  [[processors.strings.lowercase]]
    tag = "environment"
  [[processors.strings.uppercase]]
    tag = "datacenter"

# Add cluster and environment tags
[[processors.override]]
  [processors.override.tags]
    cluster = "bev-production"
    environment = "${ENVIRONMENT:-production}"
    datacenter = "${DATACENTER:-primary}"

# =============================================================================
# AGGREGATOR PLUGINS
# =============================================================================

# Create histograms for response times
[[aggregators.histogram]]
  # Reset buckets on flush
  reset = true

  # Histogram bucket settings
  buckets = [0.1, 0.5, 1.0, 2.5, 5.0, 10.0]

  # Fields to create histograms for
  fields = ["response_time", "request_duration"]

# Create basic statistics
[[aggregators.basicstats]]
  # Drop original metrics after aggregation
  drop_original = false

  # Statistics to calculate
  stats = ["count", "min", "max", "mean", "stdev", "s2", "sum"]

  # Period for aggregation
  period = "30s"